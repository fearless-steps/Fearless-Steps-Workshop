<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
	
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Fearless Steps Workshop</title>
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
	<link rel="icon" href="favicon.ico" type="image/x-icon">
	<link rel="stylesheet" href="css/styles.css"> 
	
</head>
<body>
<div class="page-wrapper">
<div class="top-banner">
<h1> Fearless Steps APOLLO:<br>A Community Resource</h1>


</div>
<div class="top-top-nav">

<!--
<div class="top-nav">
		<a class="active" href="index.html">Home</a>

		<a href="Data.html">FSC-4 Data</a>
		
		<a href="SAD.html">SAD</a>
		<a href="DIAR.html">DIAR</a>
		<a href="SID.html">SID</a>
		
		<a href="ASR.html">ASR</a>
		<a href="Conv.html">Topic Detection</a>
		<a href="ISCA_2022.html">ISCA 2022</a>
		
		<!--
		<a href="Data.html">FSC-2 Data</a>
		
		<a href="Submission.html">Submission</a>
		<a href="SAD.html">SAD</a>
		<a href="DIAR.html">DIAR</a>
		<a href="SID.html">SID</a>
		<a href="ASR.html">ASR</a>
		<a href="Final.html">Leaderboard</a>
			
	</div>
-->
<div class="article" id="Introduction">
<h2>Introduction</h2>
<p>The field of speech communications has seen extensive advancements in speech technology as well as
communication sciences for human interaction. While many speech and language resources have been
established, few exist which focus on team based problem solving in naturalistic settings. The main
challenge in establishing such a resource is the ability to capture such audio in a consistent manner, and
ensure proper associated meta-data and content knowledge is provided to allow speech/language
processing research, speech communications investigation, as well as speech technology innovation
based on machine learning advancements. Privacy constraints, as well as willingness for large teams to
be recorded with subsequent audio and meta-data shared represent the primary obstacle to overcome.<br><br>
To address this, the Fearless Steps APOLLO Community Resource, supported by NSF, is an ongoing massive
naturalistic communications resource for researchers, scientists, historians, and technology innovators to
advance their fields. Historically, the NASA Apollo program represents one of mankind’s most significant
technological challenges to place a human on the moon. This series of manned speech missions were
made possible by the dedication of an extensive group of committed scientists, engineers, and specialists
who collaborated harmoniously, showcasing an extraordinary level of teamwork. Voice communications
played a key role in ensuring a coordinated team effort, which ensure success of each mission and
leveraged team member’s strengths and scientific knowledge/experience. The speech & language
community has a substantial need for extensive Big Data speech corpora, as it is crucial for advancing
next-generation technologies in the field. The primary objective of the proposed workshop is to bring the
SPS members together to explore and address urgent needs within the speech/language community that
can advance our field through the massive naturalistic Fearless Steps APOLLO corpus. <br>
This workshop aims to feature keynote speakers, host panel discussions, present state-of-the-art research
findings, and share both resources and best practices in exploring what will be the largest publicly
available naturalistic multi-speaker team based historical audio and meta-data resource.
</div>

<!--
<div class="sub-article">
<h3>Challenge Tasks in Phase-4 (FSC P4)</h3>
<p><ul>
  										<li> <u>TASK#1</u>: Speech Activity Detection: SAD</li><br>
										
										<li> <u>TASK#2</u>: Speaker Diarization: SD</li><br>
										<ul><li> <u>2a. Track 1:</u> Diarization using system SAD</u></li><br>
										<li> <u>			2b. Track 2:</u> Diarization using reference SAD</u></li><br></ul>
										<li> <u>TASK#3</u>: Speaker Identification: SID</li><br>
										<li> <u>TASK#4</u>: Speaker Verification: SV</li><br>										
										
										<li> <u>TASK#5</u>: Automatic Speech Recognition: ASR</li><br>
									<ul>	<li> <u>			5a. Track 1:</u> ASR using reference SAD</u></li><br>
										<li> <u>			5b. Track 2:</u> ASR using system SAD</u></li><br></ul>
										<li> <u>TASK#6</u>: Topic Detection</li><br>
									<ul>	<li> <u>			6a. Track 1:</u> Topic Detection using diarized segments</u></li><br>
										<li> <u>			6b. Track 2:</u> Topic Segment using identification from audio streams</u></li><br></ul>
									</ul>
									</p>

</div>
<p>&nbsp;</p>
-->



<div class="article" id="19k_Corpus_Access">
<h2>Topics Covered</h2>
<p>
Main directions to be covered in this workshop are:<br><br>
• Big Data Recovery and Deployment; Current progress in the Fearless Steps APOLLO initiative.<br>
• Applications to Education, History, & Archival efforts.<br>
• Applications to Communication Science, Psychology (Group Dynamics/Team Cohesion).<br>
• Applications to SLT development, including but not limited to automatic speech recognition (ASR),
speech activity detection (SAD), speaker recognition, and conversational topic detection.<br>
</p>

<h3>Planned Composition </h3>
<p>
The objective of the Fearless Steps APOLLO workshop is to provide a forum for advancing speech
technology and research on massive naturalistic data. In addition, this workshop is also intended to
provide a mechanism for discussions on the latest challengesin speech/language research and innovative
solutions to these tasks. The following points provide an overview of the planned workshop and its role
in collaborative problem-solving studies.<br><br>
• Advancements in digitizing and recovery of Apollo audio from tapes, and refining audio diarization
machine learning solution(s) for community resource/sharing.<br>
• Understanding team based communication dynamics through speech processing: current
challenges and future directions.<br>
• Utility of Fearless Steps APOLLO to communication science, historical archives, and education
communities.<br>
• The FEARLESS STEPS Challenge: a tool for community engagement and data generation.<br>
• Effective worldwide deployment of the Fearless Steps APOLLO corpus and corresponding
metadata.<br><br>
The session will consist of 15-minute oral talks with:<br> (i) overview talks of the Fearless Steps APOLLO
community resource, current state-of-the-artsystems developed for the labeled datasets, and established
diarization pipeline; followed by <br>(ii) oral presentations on the best performing systems evaluated for
Fearless Steps Challenge dataset. In addition, poster sessions will also be available to researchers applying
Fearless Steps APOLLO data for novel tasks. Finally, there will be a (30-minute) panel discussion, with
panelists invited from the aforementioned communities.<br> The discussion will be moderated by the
organizers.
</p>
<p>&nbsp;</p>
</div>
<!--
<div class="article" id="News">
<h2>News</h2>
<p>Coming Soon!!</p>
<p>&nbsp;</p>
</div>
-->
<div class="article" id="Publication">
<h2>Instruction for Authors</h2>
<h3>IF YOUR ARE A PROSPECTIVE AUTHOR, VIEW THIS SECTION!</h3>
<p>We invite authors to submit their original research and workshop contributions following the guidelines outlined below. </p>
    <h3>How to Submit Your Abstract</h3>
    <p>
        Authors are invited to submit their abstracts through our dedicated submission portal available<a href="https://cmsworkshops.com/ICASSP2024/Papers/Submission.asp?Type=WS&ID=14">here</a>.
    </p>
<h3>Presentation Format</h3>
    <ul>
        <li>Based on the abstract submissions received, The presentations will be oral presentations.</li>
        <li>We encourage all forms of participation; therefore, virtual oral presentations will be facilitated for presenters who are unable to attend in person.</li>
        <li>General attendance is expected to be in-person to foster a collaborative and engaging workshop environment.</li>
        <li>Remote participation options are reserved exclusively for presenters.</li>
    </ul>

    <hr>
 

    <h3>Abstract Formatting and Requirements</h3>
    <ul>
		<p> Your submission should be a maximum of 2 pages, including references and appendices. The first page should include a brief summary of your work. The second page is reserved for figures/tables and references. The second page is optional.  </p>
        <p>Two sample PDFs are available on the website as references to guide you in formatting your submission:
            <ul>
                <li><a href="https://utdallas.box.com/s/uuw1dex8g4gdayvlmxh9cb57xxblxryu">Fearless Steps APOLLO: Advanced Naturalistic Corpora Development</a></li>
                <li><a href="https://utdallas.box.com/s/7ftt1tkubjfws1ukngoebbliaf4z8hde">DeepComboSAD: Speech Activity Detection for Naturalistic Audio streams</a></li>
            </ul>
        </p>
        
    </ul>

    <h3>Submission and Participation Policy</h3>
    <ul>
        <li>Submissions will be considered for oral presentations for the review process.</li>
        <li>While we offer remote presentation options for presenters, we aim to have an interactive and in-person experience for attendees to maximize the workshop's impact.</li>
		<li>Authors with accepted papers at ICASSP 2024 will be given an opportunity to showcase their work through
an oral presentation. Additionally, authors with original 4-page work rejected in the
ICASSP general conference will be allowed to submit for the workshop provided that the paper follows the workshop abstract submission format.</li><br>
<p>
This workshop aims to shed light on Apollo
data’s intricate nuances and potential implications, drawing the attention of researchers and engineers
towards this rich field of study. Through this, the workshop hopes to inspire robust model development
for team engagement and collaborative problem solving.</p>
    </ul>
<!--
<p>Prospective authors are encouraged to submit original 4-page full papers to ICASSP general conference in
areas related to the Fearless Steps Challenge Tasks, and other novel tasks including but not limited to
Objective experimental characterization of real scenarios in terms of <br>
i) acoustic conditions (Noise,environment change…), & <br>
ii) speech characteristics (Spontaneous speech, vocal effort…),<br>
Real data collection protocols,<br>
Data simulation algorithms,<br>
New datasets of similar domain suitable for research on robust speech processing,<br>
Performance comparison on real vs. simulated datasets for a given task and a range of methods.<br>

Authors with accepted papers at ICASSP 2024 will be given an opportunity to showcase their work through
an oral presentation or a poster session. Additionally, authors with original 4-page work rejected in the
ICASSP general conference will be allowed to submit a 2-page extended abstract in a Show-Tell format.
The posters for these extended abstracts will also be showcased in the poster session (provided it is not a
replication of any previous work using the FS Challenge data). This workshop aims to shed light on Apollo
data’s intricate nuances and potential implications, drawing the attention of researchers and engineers
towards this rich field of study. Through this, the workshop hopes to inspire robust model development
for team engagement and collaborative problem solving.

</p>.
-->
</div>
<div class="article" id="Publication">
<h2>Workshop Agenda</h2>

<h3>Submission Deadlines</h3>
    <ul>
	<li>Submission Portal: <a href="https://cmsworkshops.com/ICASSP2024/Papers/Submission.asp?Type=WS&ID=14">Submit Workshop Abstract</a> </li>
        <li>Submission Portal Opens: <strong>November 29, 2023</strong></li>
        <li>Submission Deadline: <strong style="color:red">March 1, 2024</strong></li></li>
		<li>Author Notification: <strong>March 15, 2024</strong></li></li>
		<li>Workshop Date: <strong>TBD</strong></li></li></li>
</div>
<div class="article" id="References">
<h2>References</h2>

<p>
[1] <i>Shekar, Meena M. Chandra, and John HL Hansen</i>. <b><a href="https://signalprocessingsociety.org/publications-resources/ieee-signal-processing-magazine/historical-audio-search-and-preservation">“Historical Audio Search and Preservation: Finding Waldo Within the Fearless Steps Apollo 11 Naturalistic Audio Corpus [Applications Corner]”</a></b> IEEE Signal Processing Magazine 40, no. 3: 30-38. (2023) <br><br>
[2] <i>Shekar, Meena M.C., and John HL Hansen</i>. <b><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/shekar23_interspeech.pdf">“Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data Fearless Steps Apollo-11 Audio Corpus.”</a></b> ISCA INTERSPEECH-2023 (2023).<br><br>

[3] <i>Joglekar, Aditya, Ivan Lopez-Espejo, and John H. Hansen</i>. <b><a href="https://pubs.aip.org/asa/jasa/article/153/3_supplement/A173/2885966">“Fearless Steps APOLLO: Challenges in keyword spotting and topic detection for naturalistic audio streams.”</a></b> The Journal of the Acoustical Society of America 153, no. 3: A173-A173. (2023)<br><br>

[4] <i>S.-J. Chen, J. Xie, and J. H. Hansen</i>, <b><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/chen22q_interspeech.pdf">“FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition,”</a></b> in Proc. Interspeech 2022, pp. 3058–3062 (2022)<br><br>
[5] <i>Hansen, J.H., Joglekar, A., Chen, S.J., Shekar, M.C. and Belitz, C.</i>, <b><a href="https://aclanthology.org/2022.nidcp-1.3/">“Fearless Steps APOLLO: Advanced Naturalistic Corpora Development.”</a></b> In Proceedings of the 2nd Workshop on Novel Incentives in Data Collection from People: models, implementations, challenges, and results within LREC 2022 (pp. 14-19). (2022)<br><br>
[6] <i>S.-J. Chen, W. Xia, and J. H. Hansen</i>, <b><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688225">“Scenario aware speech recognition: Advancements for Apollo Fearless Steps & CHiME-4 corpora,”</a></b> in Proc. IEEE ASRU 2021, pp. 289–295 (2021)<br><br>
[7] <i>Joglekar, Aditya, Seyed Omid Sadjadi, Meena Chandra-Shekar, Christopher Cieri, and John HL Hansen</i>. <b><a href="https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=932307">“Fearless steps challenge phase-3 (FSC-P3): Advancing SLT for Unseen Channel and Mission Data across NASA Apollo Audio.”</a></b>ISCA INTERSPEECH-2021. (2021)<br><br>
[8] <i>Joglekar, Aditya, John H.L. Hansen, M.C. Shekar, and Abhijeet Sangwan</i> (2020) <b><a href="http://www.interspeech2020.org/uploadfile/pdf/Wed-SS-2-3-5.pdf">FEARLESS STEPS Challenge (FS-2): Supervised Learning with Massive Naturalistic Apollo Data.</a></b> Proc. Interspeech 2020, 2617-2621 (2020)<br><br>
[9] <i>Hansen, John HL, Aditya Joglekar, Meena Chandra Shekhar, Vinay Kothapally, Chengzhu Yu, Lakshmish Kaushik, and Abhijeet Sangwan</i>. <b><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2019/hansen19_interspeech.pdf">“The 2019 inaugural fearless steps challenge: A giant leap for naturalistic audio.”</a></b> ISCA INTERSPEECH-2019. (2019)<br><br>
[10] <i>Yu, Chengzhu, and John HL Hansen</i>. <b><a href="https://pubs.aip.org/asa/jasa/article/141/3/1605/1056440/A-study-of-voice-production-characteristics-of">“A study of voice production characteristics of astronaut speech during Apollo 11 for speaker modeling in space.”</a></b> The Journal of the Acoustical Society of America 141, no. 3 (2017): 1605-1614<br><br>
[11] <i>Yu, Chengzhu, and John HL Hansen</i>. <b><a href="https://ieeexplore.ieee.org/document/8030331">“Active learning based constrained clustering for speaker diarization.”</a></b>IEEE/ACM Transactions on Audio, Speech, and Language Processing 25, no. 11 (2017): 2188-2198<br><br>
[12] <i>Hansen, John HL, Abhijeet Sangwan, Aditya Joglekar, Ahmet Emin Bulut, Lakshmish Kaushik, and Chengzhu Yu</i>. <b><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2018/hansen18_interspeech.pdf">“Fearless Steps: Apollo-11 Corpus Advancements for Speech Technologies from Earth to the Moon.”</a></b> In INTERSPEECH, pp. 2758-2762. (2018)<br><br>
[13] <i>Yu, Chengzhu, John HL Hansen, and Douglas W. Oard</i>. <b><a href="https://www.isca-speech.org/archive/interspeech_2014/yu14_interspeech.html">“'Houston, we have a solution': A case study of the analysis of astronaut speech during NASA Apollo 11 for long-term speaker modeling.”</a></b> In Fifteenth Annual Conference of the International Speech Communication Association. 2014<br><br>
[14] <i>Ziaei, Ali, Lakshmish Kaushik, Abhijeet Sangwan, John HL Hansen, and Douglas W. Oard</i>. <b><a href="https://terpconnect.umd.edu/~oard/pdf/interspeech14-ziaei.pdf">“Speech activity detection for NASA Apollo space missions: Challenges and solutions.”</a></b> In Fifteenth Annual Conference of the International Speech Communication Association. 2014<br><br>
[15] <i>Sangwan, Abhijeet, Lakshmish Kaushik, Chengzhu Yu, John HL Hansen, and Douglas W. Oard</i>. <b><a href="https://www.isca-speech.org/archive/interspeech_2013/sangwan13_interspeech.html">“'Houston, we have a solution': using NASA Apollo program to advance speech and language processing technology.”</a></b> In INTERSPEECH, pp. 1135-1139. 2013<br><br>

</p>

</p>	
<p>
[1] <i>T. Vuong, N. Madaan, R. Panda, and R. M. Stern</i>, <b>“Investigating the important temporal modulations for deep-learning-based speech activity detection,”</b> in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 525–531.<br><br>
[2] <i>P. Gimeno, D. Ribas, A. Ortega, A. Miguel, and E. Lleida</i>, <b>“Unsupervised adaptation of deep speech activity detection models to unseen domains,”</b> Applied Sciences, vol. 12, no. 4, p. 1832, 2022.<br><br>
[3] <i>V. Pannala and B. Yegnanarayana</i>, <b>“A neural network approach for speech activity detection for Apollo corpus,”</b> Computer Speech & Language, vol. 65, p. 101137, 2021.<br><br>
[4] <i>W. Wang, D. Cai, J. Wang, Q. Lin, X. Wang, M. Hong, and M. Li</i>, <b>“The DKU-Duke-Lenovo System Description for the Fearless Steps Challenge Phase III,”</b> in ISCA Interspeech, 2021, pp. 1044–1048.<br><br>
[5] <i>T. Vuong, Y. Xia, and R. M. Stern</i>, <b>“The Application of Learnable STRF Kernels to the 2021 Fearless Steps Phase-03 SAD Challenge,”</b> in ISCA Interspeech, 2021, pp. 4364–4368.<br><br>
[6] <i>P. Gimeno, A. Ortega, A. Miguel, and E. Lleida</i>, <b>“Unsupervised Representation Learning for Speech Activity Detection in the Fearless Steps Challenge 2021,”</b> in ISCA Interspeech 2021, 2021, pp. 4359–4363.<br><br>
[7] <i>O. Ghahabi and V. Fischer</i>, <b>“EML Online Speech Activity Detection for the Fearless Steps Challenge Phase-III,”</b> in ISCA Interspeech 2021, 2021, pp. 4379–4382.<br><br>
[8] <i>P. Gimeno, D. Ribas, A. Ortega, A. Miguel, and E. Lleida</i>, <b>“Convolutional recurrent neural networks for speech activity detection in naturalistic audio from Apollo missions,”</b> Proc. IberSPEECH, vol. 2021, pp. 26–30, 2021.<br><br>
[9] <i>A. Gorin, D. Kulko, S. Grima, and A. Glasman</i>, <b>“'This is Houston. Say again, please'. The Behavox System for the Apollo-11 Fearless Steps Challenge (Phase II),”</b> ISCA Interspeech 2020, pp. 2612–2616.<br><br>
[10] <i>J. Heitkaemper et al.</i>, <b>“Statistical and Neural Network Based Speech Activity Detection in Non-Stationary Acoustic Environments,”</b> in ISCA Interspeech 2020, 2020, pp. 2597–2601.<br><br>
[11] <i>X. Zhang, W. Wang, and P. Zhang</i>, <b>“Speaker Diarization System Based on DPCA Algorithm for Fearless Steps Challenge Phase-2,”</b> in ISCA Interspeech 2020, 2020, pp. 2602–2606.<br><br>
[12] <i>Q. Lin, T. Li, and M. Li</i>, <b>“The DKU Speech Activity Detection and Speaker Identification Systems for Fearless Steps Challenge Phase-02,”</b> in ISCA Interspeech 2020, 2020, pp. 2607–2611.<br><br>
[13] <i>A. Ragano, E. Benetos, A. Hines et al.</i>, <b>“Development of a speech quality database under uncontrolled conditions,”</b> 2020.<br><br>
[14] <i>B. Sharma, R. K. Das, and H. Li</i>, <b>“Multi-level adaptive speech activity detector for speech in naturalistic environments,”</b> ISCA Interspeech 2019, pp. 2015–2019.<br><br>
[15] <i>A. Vafeiadis et al.</i>, <b>“Two-dimensional convolutional recurrent neural networks for speech activity detection,”</b> in ISCA Interspeech 2019. ISCA, 2019.<br><br>
[16] <i>G. Deshpande, V. S. Viraraghavan, R. Gavas</i>, <b>“A successive difference feature for detecting emotional valence from speech,”</b> in Proc. SMM19, Workshop on Speech, Music and Mind 2019, 2019, pp. 36–40.<br><br>

</p>				
  						
  <p>&nbsp;</p>						

</div>
<!--
<div class="article" id="Contact_us">
<h2>Contact Us</h2>
<p>For further questions or inquiries, Please do not hesitate to contact us,</p>
<p><a href="mailto:FearlessSteps@utdallas.edu">Fearless Steps</a></p>
<p>&nbsp;</p>


</div>
<!--
<div class="article" id="Inaugural">
<h2>Inaugural FS-1 Challenge</h2>
<p>For information on the Fearless steps Challenge Phase-01, Please<a href="http://fearlesssteps.exploreapollo.org/">click here</a></p>
<p>&nbsp;</p>
</div>

<div class="article" id="Disclaimer">
<h2>Additional Information</h2>
<p>The Fearless Steps corpus is derived fom a five-year NSF CISE funded project awarded to CRSS at the University of Texas at Dallas. UTDallas-CRSS established the hardware/software solutions to digitize and diarize 19,000 hrs of NASA Apollo data. All core Apollo data released as part of this challenge has been approved for public release by NASA Export Control. The full audio corpus is also available through UTDallas-CRSS. Any reference to or listing of organizations other than UTD is for information only; it does not imply recommendation or endorsement by UTDallas-CRSS nor does it imply that the products mentioned are necessarily the best available for that purpose. </p>

<div class="sub-article">
<h3>Licensing</h3>
<h4>Details</h4>
<p>All the conversations between Astronauts and and Mission Control Personnel during the Apollo-11 Mission were recorded by NASA. The tireless efforts of CRSS-UTD transcribers and researchers contributed to the shaping of this enormous amounts of data into a well-defined corpus to address various speech and language tasks for naturalistic audio, a portion of which is now made publicly available to the speech community through this Challenge via a creative commons license.<br>
<br><strong>Note:</strong>The Creative Commons License is restricted to the efforts made by CRSS-UTD, which involves 100 hours of Challenge Corpus (audio) data sampled from 8Khz, along with its meta-data generated separately. The license also covers all the scripts which were used in the preparation of the corpus and systems built to support the tasks in this Challenge, along with the webpages developed to host the Challenge. </p>

<h4>NASA Media Usage Guidelines</h4>
<p> NASA content - images, audio, video, and computer files used in the rendition of 3-dimensional models, such as texture maps and polygon data in any format - generally are not copyrighted. You may use this material for educational or informational purposes, including photo collections, textbooks, public exhibits, computer graphical simulations and Internet Web pages. This general permission extends to personal Web pages.</p>
<p> 
News outlets, schools, and text-book authors may use NASA content without needing explicit permission. NASA content used in a factual manner that does not imply endorsement may be used without needing explicit permission. NASA should be acknowledged as the source of the material. NASA occasionally uses copyrighted material by permission on its website. Those images will be marked copyright with the name of the copyright holder. NASA's use does not convey any rights to others to use the same material. Those wishing to use copyrighted material must contact the copyright holder directly.</p>
<h4>Creative Commons License</h4>

<p>
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a>
<span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Dataset" property="dct:title"rel="dct:type">FEARLESS STEPS CHALLENGE</span>by<a xmlns:cc="http://creativecommons.org/ns#" href="https://fearless-steps.github.io/ChallengePhase2/" property="cc:attributionName" rel="cc:attributionURL">Aditya Joglekar, John H.L. Hansen</a>is licensed under a<a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a><br>Based on a work at<a xmlns:dct="http://purl.org/dc/terms/" href="https://www.nasa.gov/mission_pages/apollo/apollo-11.html" rel="dct:source">https://www.nasa.gov/mission_pages/apollo/apollo-11.html</a><br>
Permissions beyond the scope of this license may be available at<a xmlns:cc="http://creativecommons.org/ns#" href="https://www.nasa.gov/multimedia/guidelines/index.html" rel="cc:morePermissions">https://www.nasa.gov/multimedia/guidelines/index.html</a>
</p>
<p>For Additional Information regarding Commercial and Non-Commercial Use:<br>
Please visit:<a href="https://www.nasa.gov/multimedia/guidelines/index.html"> https://www.nasa.gov/multimedia/guidelines/index.html</a></p>
</div>
<p>&nbsp;</p>
</div>
<div class="article" id="organizers">
<h3>Organizers</h3>
<ul>
<li>
John H.L. Hansen, University of Texas at Dallas</li>
<li>
Christopher Cieri, University of Pennsylvania</li>
<li>
Omid Sadjadi, NIST</li>
<li>
Aditya Joglekar, University of Texas at Dallas</li>
<li>Meena Chandra Shekar, University of Texas at Dallas</li>
<li>Abhijeet Sangwan, University of Texas at Dallas</li>

</ul>

<h3>Acknowledgements</h3>
<p>This project was supported in part by  AFRL  under  contractFA8750-15-1-0205, NSF-CISE Project 1219130, and partially by the University of Texas at Dallas from the DistinguishedUniversity Chair in Telecommunications Engineering held by J.H. L. Hansen. We would also like to thank Tatiana Korelsky and the National Science Foundation (NSF) for their support on this scientific and historical project.
A special Thanks to Katelyn Foxworth for leading the ground-truth development efforts for the FS-02 and FS-03 Challenge Corpus.</p>
</div>-->
</div>
</div>
</body>


</html>
